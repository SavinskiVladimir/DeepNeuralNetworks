{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Библиотеки",
   "id": "ad47cf725b88c7d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:15:45.707115Z",
     "start_time": "2026-02-11T13:15:40.897812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ],
   "id": "8c97a8defbfba0db",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Механизм внимания",
   "id": "fc98f821ab4dc466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:15:51.201989Z",
     "start_time": "2026-02-11T13:15:51.193760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Q (Query) - что ищу\n",
    "        # K (Key) - что я предлагаю\n",
    "        # V (Value) - какую информацию отдаю\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # насколько токен i связан с токеном j\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, seq, seq)\n",
    "\n",
    "        # масштабирование для избежания насыщения градиента\n",
    "        scores = scores / math.sqrt(d_k)\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # взвешенная сумма V\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        return output, attn_weights\n"
   ],
   "id": "6a0e13263151768c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:28:43.602249Z",
     "start_time": "2026-02-11T13:28:43.586467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# пример токенизированного текста\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "embed_dim = 4\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "print(\"Input x:\")\n",
    "print(x)\n",
    "print(\"Shape:\", x.shape)"
   ],
   "id": "f00486f7da45a848",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      "tensor([[[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
      "         [-1.1229, -0.1863,  2.2082, -0.6380],\n",
      "         [ 0.4617,  0.2674,  0.5349,  0.8094]]])\n",
      "Shape: torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:28:59.958973Z",
     "start_time": "2026-02-11T13:28:59.940008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Получение Q, K, V из токенизированного текста\n",
    "W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "Q = W_q(x)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(\"\\nQ:\")\n",
    "print(Q)\n",
    "print(\"\\nK:\")\n",
    "print(K)\n",
    "print(\"\\nV:\")\n",
    "print(V)"
   ],
   "id": "bde5c164afc656a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:\n",
      "tensor([[[-0.2649, -0.0397,  0.1738,  0.0548],\n",
      "         [ 0.3662,  1.3071, -1.0046,  0.0585],\n",
      "         [-0.5627, -0.2125,  0.3637,  0.0456]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "K:\n",
      "tensor([[[ 0.1799,  0.1574, -0.1143,  0.0052],\n",
      "         [ 0.0543,  0.2965, -1.1980,  0.5401],\n",
      "         [ 0.4996,  0.3018, -0.3854,  0.1773]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "V:\n",
      "tensor([[[ 0.0620, -0.0369,  0.0671,  0.0050],\n",
      "         [ 0.9148,  0.4785,  0.9522,  1.2772],\n",
      "         [-0.0075, -0.1336,  0.0248,  0.1580]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:32:55.308677Z",
     "start_time": "2026-02-11T13:32:55.269302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пример применения реализованного механизма внимания\n",
    "attntn = ScaledDotProductAttention()\n",
    "output, attn_weights = attntn.forward(Q=Q, K=K, V=V)\n",
    "print(attn_weights)"
   ],
   "id": "cc08e092b2094d13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3479, 0.3258, 0.3263],\n",
      "         [0.2372, 0.4445, 0.3183],\n",
      "         [0.3692, 0.3133, 0.3176]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Multi-head attention",
   "id": "4120b3919e4be185"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:59:03.459562Z",
     "start_time": "2026-02-11T13:59:03.446334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Линейные преобразования для Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "        # Финальная проекция\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Q, K, V\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # reshape для multi-head\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (batch, heads, seq, head_dim)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # применяем внимание\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "\n",
    "        # объединяем головы\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "    \n",
    "        return output\n"
   ],
   "id": "6c2d9c9fc51c24d6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверка на случайных данных",
   "id": "92eb3b2d7e4a283a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:59:41.928061Z",
     "start_time": "2026-02-11T13:59:41.894076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "output, attn_weights = mha.forward(x, return_attention=True)\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)\n",
    "print(torch.allclose(x, output))\n",
    "print(attn_weights.shape)\n",
    "print(attn_weights[0, 0])\n",
    "\n"
   ],
   "id": "f76ec1d664fd07b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 16])\n",
      "torch.Size([2, 5, 16])\n",
      "False\n",
      "torch.Size([2, 4, 5, 5])\n",
      "tensor([[0.2267, 0.2042, 0.2597, 0.1530, 0.1564],\n",
      "        [0.2125, 0.2126, 0.2224, 0.1817, 0.1708],\n",
      "        [0.1179, 0.1815, 0.1725, 0.2948, 0.2331],\n",
      "        [0.1439, 0.1855, 0.2088, 0.2448, 0.2171],\n",
      "        [0.1913, 0.2026, 0.2247, 0.1939, 0.1874]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:01:00.670477Z",
     "start_time": "2026-02-11T14:01:00.594720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x2 = x.clone()\n",
    "x2[0, 0] += 10.0   # меняем один токен для проверки взаимного влияния\n",
    "\n",
    "out1 = mha.forward(x)\n",
    "out2 = mha.forward(x2)\n",
    "\n",
    "print(\"difference in other positions:\",\n",
    "      torch.norm(out1[0, 1:] - out2[0, 1:]))"
   ],
   "id": "f788f27499f2eee2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference in other positions: tensor(9.5380, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Интеграция в простую модель\n",
    "\n",
    "Embedding — «кто я»\n",
    "Attention — «кто рядом и как он на меня влияет»\n",
    "Pooling — «общее впечатление»\n",
    "Classifier — «решение»"
   ],
   "id": "615cd2feae8b9231"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:16:11.118300Z",
     "start_time": "2026-02-11T13:16:11.110525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # pooling\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "\n",
    "        return self.classifier(x)\n"
   ],
   "id": "6ce156e9c65330ec",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Проверка работы модели",
   "id": "847fb2e5cc841a27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:20:51.417287Z",
     "start_time": "2026-02-11T13:20:51.372122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# параметры\n",
    "vocab_size = 100\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "num_classes = 3\n",
    "seq_len = 8\n",
    "batch_size = 10\n",
    "\n",
    "model = SimpleAttentionModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# случайный \"текст\"\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# прямой проход\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Input tokens:\\n\", x)\n",
    "print(\"Logits:\\n\", logits)\n",
    "print(\"Predicted classes:\\n\", preds)\n"
   ],
   "id": "8b860352ef33a42c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens:\n",
      " tensor([[ 9, 27, 38, 83, 65, 34, 73, 26],\n",
      "        [88,  2, 46, 21, 83, 67, 72, 43],\n",
      "        [47,  2, 71, 52, 19, 21, 95, 93],\n",
      "        [ 3, 29, 40, 38,  5,  0, 66, 60],\n",
      "        [51, 81, 81, 21, 79, 94,  7, 76],\n",
      "        [44, 96, 60, 19, 49, 12, 14, 74],\n",
      "        [48, 45,  6, 66, 24, 52, 77, 83],\n",
      "        [71, 29, 20, 54,  1, 62, 83, 40],\n",
      "        [17, 75, 36, 98, 27, 89, 52, 28],\n",
      "        [10, 98,  7, 89, 30,  2, 66, 80]])\n",
      "Logits:\n",
      " tensor([[ 0.1607,  0.2840, -0.0180],\n",
      "        [ 0.1302,  0.3509, -0.0536],\n",
      "        [ 0.0769,  0.4199,  0.1015],\n",
      "        [ 0.1840,  0.2547, -0.2494],\n",
      "        [ 0.0160,  0.4413, -0.0203],\n",
      "        [ 0.1213,  0.2651, -0.0520],\n",
      "        [ 0.1867,  0.1728, -0.2080],\n",
      "        [ 0.2382,  0.2816,  0.1088],\n",
      "        [ 0.1682,  0.2868,  0.0510],\n",
      "        [ 0.1238,  0.3252, -0.0445]])\n",
      "Predicted classes:\n",
      " tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T13:20:39.403308Z",
     "start_time": "2026-02-11T13:20:39.399659Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e1fd8063f1ff063d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ecfb36dd7b4ae1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
